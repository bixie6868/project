## An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA（PICa）
### 动机：   
* 现存的方法都是分为两个阶段，1、从外部资源检索知识2、根据检索到的知识、输入图片和问题进行融合推理，进行答案预测   
**问题：** 检索到的知识可能有噪音或和问题不相关，同时re-embedded的知识特征可能背离知识库中原始的含义。此外，学习一个健壮的知识-图像-问题联合表征需要足够的训练数据，因此很难迁移到新的问题类型。
### 文章主要方案：
* 总得来说：是将上述知识检索和推理步骤隐式的统一起来的一种简单而有效的方法
  - 将image转换为captions，让CPT-3可以理解（过程中探究 什么样的**text formats**可以更好的描述图片的内容）
  - 提供in-context VQA examples（过程中探究 如何更好的选择和使用上下文中的示例）
    ![Pica](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-28_10-27-48.png "Pica")
### 具体实现：
* 使用CLIP模型进行相似度计算，选取样例。
* Multi-query ensemble多查询集成
* ![Pica](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-28_10-32-56.png "Pica")
### 实验：
1. 和现有的传统VQA实验结果对比  
2. 设置不同的shot，在PICa-Base及PICa-Full上的实验结果（Caption/Caption+Tags）   
3. 验证实验加入图片（caption）意义，将image表示成不同的文本形式效果对比：
    - Blind：空白的string表示image，样例值放问题，不添加图片
    - Tags： 将图像表示为由自动标记模型预测的标签列表
    - VinVL-Caption-COCO：在COCO training set上fine-tune VinVL-Base
    - VinVL-Caption-CC : 在Conceptual Captions数据上训练VinVL-base
    - API-Caption ：使用Microsoft Azure tagging API
    - GT-Caption ：给定COCO原本的Caption
    - Caption+Tags ：链接图片的caption以及tag list标签列表   
4. 选择不同的examples的方式（CLIP model用于相似度计算）
    - Random
    - Question
    - QuestionDissimilar
    - Question+Answer（oracle）
    - Image only
    - Image + Question（文章中使用 PICa-Full）
5. 多查询集成实验，n*k个样例得到k个prompt，每个prompt包含n个例子，得到k个预测的答案，选择log可能值最高的作为最终的答案
6. 以及在VQAv2上的结果，虽然效果不如Oscar好，指明未来方向"期望end-to-end的视觉编码器能帮助更好地回答这些问题"
### 启发
* PICa 其实很简单，但是由于“首次使用GPT-3做多模态的任务”，并且效果在OK-VQA数据集上有很大提升，论文中也进行了多个实验来支撑整篇文章，内容丰满，所有有一定的影响力。
### 借鉴价值
* 文章主要就4个点，1、图片用Caption表示，2、图片通过tagging API获得到的Tags，3、选取不同样例多次查询，选择最好答案，4、example的选择。个人感觉infoseek的数据集关注的是更细粒度的问题，每个问题的关注点不一样，所以选取一些样例做few-shots可能效果不会很好，如户保田GPT-4V上few-shots效果会差一些。
* 尝试使用把问题拆解成两个问题，问题是“这个建筑所在的街道的地址是什么”，将问题拆解为，先问“这个建筑是什么”，再问“这个教堂在哪条街道”。看样例。或者根据这个“教堂”去检索相关的知识，拼接到prompt上。
## PROMPTCAP: Prompt-Guided Image Captioning for VQA with GPT-3（ICCV 2023）
### 动机：
图片的Caption所能表示的信息不全，通常会遗漏LM回答问题所需要的视觉细节
### 文章贡献：
* 对于忽略回答问题所需要的图片细节的问题
  - 提出了PromptCap,来让模型生成和问题相关的文本描述
* 如何训练PromptCap，来让其生成与问题相关的文本描述？训练数据从哪来呢？
  - 提出了使用GPT-3的in-context learning能力构建这样的数据。其数据格式为<I, P, C_q>，I为图像，P为基于问题的prompt，C_q为与问题相关的文本描述
  - ![cap](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-29_09-51-17.png "PromptCap")
  - GPT-3生成训练数据的过程：
  - ![cap](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-29_10-41-01.png "cap")
  - 解析：COCO中每个图像都有人工标注的5个文本，VQAv2数据集中每张图像都有一个问题和一个答案。最关键的一点来了，VQAv2数据集的图像都来源于COCO，通过对应关系就可以得到这样的数据格式<I, C, Q, A>，I为图像，C为常规的文本描述，Q为问题，A为答案。
### 实验过程：
* ![cap](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-29_16-29-32.png "cap")
### 借鉴价值：
* 文章提出了一种更好的表示图片的方式，可以尝试在infoseek上作为辅助信息使用，并且PromptCap在huggingface上可以调用
## Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA(PROOFREAD)
### 动机：
* 现存的方法采用大语言模型（LLM）中的隐性知识来取得优异的效果，但认为现有方法可能存在对图像的理解的偏见和知识不足的问题。
### 文章贡献：
* Answer Prediction Module：
  - 将问题P（包括图片和问题）作为输入，但只是这些内容缺少知识，因此在此基础上会将Knowledge和question进行拼接（blip-2 flan t5 xxl)
* Knowledge Generation Module：
  - 输入问题P，使用Mv model 来编码问题和图片，在Demo Bank中检索相似样本，构建这些demonstrations和问题作为prompt生成问题Q，将生成的问题送入Mk生成对应的知识K
  - 评估Knowledge的质量，分为Useful Knowledge，Harmful Knowledge以及Neutural Knowledge
* Knowledge Filter Module：
  - 利用XGBoost这个渐变增强决策树（GBDT）作为知识感知器Mf来分类知识
![PROOFREAD](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-01_19-51-07.png "proo")
* 模型首先构建了Dome Bank，从train中选取3k样例进行构建，经过Mq产生一些与Dome Bank样例相关的问题GPT-3，通过Mk生成答案作为知识，将问题P和知识拼接送入blip-2生成答案，再通过Mf过滤不需要的知识得到最终答案。
### 思考：
* 分成两个问题来问时，第一个问题的正确与否 很关键（是否有什么手段）判断当前的实体答案是否准确？？？是否可以采用上述方式使用两个LLM 来修正答案
* 该论文选取相似样本的方式也值得借鉴，用得到的答案再去更新原始的Demo Bank
## Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions(Tsinghua,EMNLP2023 Findings)
### 动机：
* 解决图片和Caption之间的差异，最终阻碍最后的推理工作。
### 文章贡献：
* 设计了一个框架（根据Caption和问题-->回答该问题所需要的新问题）
* 修正不准确的信息，抽出useful information
![th](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-04_11-29-03.png "th")
### 借鉴价值：
* 使用LLM对于当前问题，产生多个可以回答当前问题的问题，拼接知识辅助回答

## Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering
### 主要内容：
* 探究以下每个因素对VQA任务性能的影响
  - 问题模板的选择
  - 包含几个示例
  - 加入Caption作为额外的视觉信息
  - 加入chain-of-thought
* 结论：合适的问题模板，额外的视觉信息（如Captions）可以提升VQA任务性能，特别是和一些样例结合使用时。COT可能会影响VQA的准确性
### 问题模板的选择，对VQA任务性能的影响
* 问题模板：
  ![tem](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-08_10-20-21.png "tem")
* 结果：
  -可以看出经过指令调整的BLIP2-FLAN-T5模型显示出对template变化的中等敏感性,而OPT根据所选模板显示出相当大的性能差异。
  ![result](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-08_10-23-00.png "res")
### few-shot和Caption，对VQA性能影响
* 虽然问题可以指导模型回答,但它们很难精确地引导模型走向所需的**回答格式**。因此，使用纯文本exemplars将为模型提供更准确的指导，从而提高性能。
* 结论：
  - 虽然在标准QA设置中few-shot不起作用,但它们在Caption + QA和CoT + QA设置中显示出有效性。猜测：在没有对应图像的情况下（仅提供文本示例），引入更多的示例会引入噪声
  - GPT-4V few-shot性能下降是不是因为图片没有加，样例引入了噪音？
![res](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-08_10-55-05.png "res")
### COT,对VQA任务影响
* 结论：BLIP2中COT思维链的有效性有限
![res](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-08_11-27-35.png "res")
### 借鉴意义
* 现在遇到的问题是：我们使用Gemini去是生成Caption，到底是few-shot并且添加图片的效果会好（图片怎么加），还是few-shot不加图片（都不加/只最后一张加）？
* GPT-4V那篇文章同样遇到了GPT-4V图片输入的限制，该论文few-shot的输入形式如下：
![gpt4v](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-08_11-40-25.png "gpt4")
* 跟我们目前遇到问题一样，上述是他们的解决方式，few-shot性能下降，也可能是他们论文中few-shot的方式不利于学习。

## Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment(Baotian Hu,Harbin Institute of Technology)
### 动机
* Q-former以及MLP ：广泛使用的视觉语言映射方法，侧重于图像文本描述的对齐
* **忽略** 视觉知识的对齐，即将视觉效果与相关知识联系起来
### 文章贡献：
* VKA（视觉知识对齐）：对其进行图像知识对训练，实现视觉知识的获取和投影 （image-knowledge 对 收集自Wikipedia）
* FKA（细粒度知识适配器）：提取图像的细粒视觉知识并将其注入大语言模型，从VKA生成的错综复杂的视觉知识表示中提取有价值的信息
* 结论：文章提出的CVLM显著提高了基于知识的VQA的LMM性能
### 方法实现：
* 使用了一个预训练的**小型自回归语言模型**（OPT-1.3B）作为视觉知识的生成器，该模型通过在每个块中添加交叉注意力层与视觉序列 hI 进行交互。
* 使用预训练的视觉知识生成器作为backbone，构建整个视觉-知识对齐器，类似于 Q-former。
  ![CVLM](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-02-27_10-25-27.png "CVLM")

  

