## An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA（PICa）
### 动机：   
现存的方法都是分为两个阶段，1、从外部资源检索知识2、根据检索到的知识、输入图片和问题进行融合推理，进行答案预测   
**问题：** 检索到的知识可能有噪音或和问题不相关，同时re-embedded的知识特征可能背离知识库中原始的含义。此外，学习一个健壮的知识-图像-问题联合表征需要足够的训练数据，因此很难迁移到新的问题类型。
### 文章主要方案：
* 总得来说：是将上述知识检索和推理步骤隐式的统一起来的一种简单而有效的方法
  - 将image转换为captions，让CPT-3可以理解（过程中探究 什么样的**text formats**可以更好的描述图片的内容）
  - 提供in-context VQA examples（过程中探究 如何更好的选择和使用上下文中的示例）
### 具体实现：
* 使用CLIP模型进行相似度计算，选取样例。
* Multi-query ensemble多查询集成
### 实验：
1、和现有的传统VQA实验结果对比  
2、设置不同的shot，在PICa-Base及PICa-Full上的实验结果（Caption/Caption+Tags）   
3、验证实验加入图片（caption）意义，将image表示成不同的文本形式效果对比：
  - Blind：空白的string表示image，样例值放问题，不添加图片
  - Tags： 将图像表示为由自动标记模型预测的标签列表
  - VinVL-Caption-COCO：在COCO training set上fine-tune VinVL-Base
  - VinVL-Caption-CC : 在Conceptual Captions数据上训练VinVL-base
  - API-Caption ：使用Microsoft Azure tagging API
  - GT-Caption ：给定COCO原本的Caption
  - Caption+Tags ：链接图片的caption以及tag list标签列表
4、选择不同的examples的方式
  - Random
  - Question
  - QuestionDissimilar
  - Question+Answer（oracle）
  - Image only
  - Image + Question（文章中使用 PICa-Full）
5、多查询集成实验，n*k个样例得到k个prompt，每个prompt包含n个例子，得到k个预测的答案，选择log可能值最高的作为最终的答案
### 启发
PICa 其实很简单，但是由于首次使用GPT-3，并且效果有提升，所有实验效果很不错   ，多次查询
