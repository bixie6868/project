## An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA（PICa）
### 动机：   
* 现存的方法都是分为两个阶段，1、从外部资源检索知识2、根据检索到的知识、输入图片和问题进行融合推理，进行答案预测   
**问题：** 检索到的知识可能有噪音或和问题不相关，同时re-embedded的知识特征可能背离知识库中原始的含义。此外，学习一个健壮的知识-图像-问题联合表征需要足够的训练数据，因此很难迁移到新的问题类型。
### 文章主要方案：
* 总得来说：是将上述知识检索和推理步骤隐式的统一起来的一种简单而有效的方法
  - 将image转换为captions，让CPT-3可以理解（过程中探究 什么样的**text formats**可以更好的描述图片的内容）
  - 提供in-context VQA examples（过程中探究 如何更好的选择和使用上下文中的示例）
    ![Pica](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-28_10-27-48.png "Pica")
### 具体实现：
* 使用CLIP模型进行相似度计算，选取样例。
* Multi-query ensemble多查询集成
* ![Pica](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-28_10-32-56.png "Pica")
### 实验：
1. 和现有的传统VQA实验结果对比  
2. 设置不同的shot，在PICa-Base及PICa-Full上的实验结果（Caption/Caption+Tags）   
3. 验证实验加入图片（caption）意义，将image表示成不同的文本形式效果对比：
    - Blind：空白的string表示image，样例值放问题，不添加图片
    - Tags： 将图像表示为由自动标记模型预测的标签列表
    - VinVL-Caption-COCO：在COCO training set上fine-tune VinVL-Base
    - VinVL-Caption-CC : 在Conceptual Captions数据上训练VinVL-base
    - API-Caption ：使用Microsoft Azure tagging API
    - GT-Caption ：给定COCO原本的Caption
    - Caption+Tags ：链接图片的caption以及tag list标签列表   
4. 选择不同的examples的方式（CLIP model用于相似度计算）
    - Random
    - Question
    - QuestionDissimilar
    - Question+Answer（oracle）
    - Image only
    - Image + Question（文章中使用 PICa-Full）
5. 多查询集成实验，n*k个样例得到k个prompt，每个prompt包含n个例子，得到k个预测的答案，选择log可能值最高的作为最终的答案
6. 以及在VQAv2上的结果，虽然效果不如Oscar好，指明未来方向"期望end-to-end的视觉编码器能帮助更好地回答这些问题"
### 启发
* PICa 其实很简单，但是由于“首次使用GPT-3做多模态的任务”，并且效果在OK-VQA数据集上有很大提升，论文中也进行了多个实验来支撑整篇文章，内容丰满，所有有一定的影响力。
### 借鉴价值
* 文章主要就4个点，1、图片用Caption表示，2、图片通过tagging API获得到的Tags，3、选取不同样例多次查询，选择最好答案，4、example的选择。个人感觉infoseek的数据集关注的是更细粒度的问题，每个问题的关注点不一样，所以选取一些样例做few-shots可能效果不会很好，如户保田GPT-4V上few-shots效果会差一些。尝试使用把问题拆解成两个问题，问题是“这个建筑所在的街道的地址是什么”，将问题拆解为，先问“这个建筑是什么”，再问“这个教堂在哪条街道”。看样例。或者根据这个“教堂”去检索相关的知识，拼接到prompt上。
## PROMPTCAP: Prompt-Guided Image Captioning for VQA with GPT-3（ICCV 2023）
### 动机：
图片的Caption所能表示的信息不全，通常会遗漏LM回答问题所需要的视觉细节
### 文章贡献：
* 对于忽略回答问题所需要的图片细节的问题
  - 提出了PromptCap,来让模型生成和问题相关的文本描述
* 如何训练PromptCap，来让其生成与问题相关的文本描述？训练数据从哪来呢？
  - 提出了使用GPT-3的in-context learning能力构建这样的数据。其数据格式为<I, P, C_q>，I为图像，P为基于问题的prompt，C_q为与问题相关的文本描述
  - ![cap](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-29_09-51-17.png "PromptCap")
  - GPT-3生成训练数据的过程：
  - ![cap](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-29_10-41-01.png "cap")
  - 解析：COCO中每个图像都有人工标注的5个文本，VQAv2数据集中每张图像都有一个问题和一个答案。最关键的一点来了，VQAv2数据集的图像都来源于COCO，通过对应关系就可以得到这样的数据格式<I, C, Q, A>，I为图像，C为常规的文本描述，Q为问题，A为答案。
### 实验过程：
* ![cap](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-29_16-29-32.png "cap")
### 借鉴价值：
* 文章提出了一种更好的表示图片的方式，可以尝试在infoseek上作为辅助信息使用，并且PromptCap在huggingface上可以调用
## Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA(PROOFREAD)
### 动机：
* 现存的方法采用大语言模型（LLM）中的隐性知识来取得优异的效果，但认为现有方法可能存在对图像的理解的偏见和知识不足的问题。
### 文章贡献：
* Answer Prediction Module：
  - 将问题P（包括图片和问题）作为输入，但只是这些内容缺少知识，因此在此基础上会将Knowledge和question进行拼接（blip-2 flan t5 xxl)
* Knowledge Generation Module：
  - 输入问题P，使用Mv model 来编码问题和图片，在Demo Bank中检索相似样本，构建这些demonstrations和问题作为prompt生成问题Q，将生成的问题送入Mk生成对应的知识K
  - 评估Knowledge的质量，分为Useful Knowledge，Harmful Knowledge以及Neutural Knowledge
* Knowledge Filter Module：
  - 利用XGBoost这个渐变增强决策树（GBDT）作为知识感知器Mf来分类知识
![PROOFREAD](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-01_19-51-07.png "proo")
* 模型首先构建了Dome Bank，从train中选取3k样例进行构建，经过Mq产生一些与Dome Bank样例相关的问题GPT-3，通过Mk生成答案作为知识，将问题P和知识拼接送入blip-2生成答案，再通过Mf过滤不需要的知识得到最终答案。 
