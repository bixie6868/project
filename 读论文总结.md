## An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA（PICa）
### 动机：   
现存的方法都是分为两个阶段，1、从外部资源检索知识2、根据检索到的知识、输入图片和问题进行融合推理，进行答案预测   
**问题：** 检索到的知识可能有噪音或和问题不相关，同时re-embedded的知识特征可能背离知识库中原始的含义。此外，学习一个健壮的知识-图像-问题联合表征需要足够的训练数据，因此很难迁移到新的问题类型。
### 文章主要方案：
* 总得来说：是将上述知识检索和推理步骤隐式的统一起来的一种简单而有效的方法
  - 将image转换为captions，让CPT-3可以理解（过程中探究 什么样的**text formats**可以更好的描述图片的内容）
  - 提供in-context VQA examples（过程中探究 如何更好的选择和使用上下文中的示例）
    ![Pica](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-28_10-27-48.png "Pica")
### 具体实现：
* 使用CLIP模型进行相似度计算，选取样例。
* Multi-query ensemble多查询集成
* ![Pica](https://github.com/bixie6868/project/blob/main/images/Snipaste_2023-12-28_10-32-56.png "Pica")
### 实验：
1. 和现有的传统VQA实验结果对比  
2. 设置不同的shot，在PICa-Base及PICa-Full上的实验结果（Caption/Caption+Tags）   
3. 验证实验加入图片（caption）意义，将image表示成不同的文本形式效果对比：
  - Blind：空白的string表示image，样例值放问题，不添加图片
  - Tags： 将图像表示为由自动标记模型预测的标签列表
  - VinVL-Caption-COCO：在COCO training set上fine-tune VinVL-Base
  - VinVL-Caption-CC : 在Conceptual Captions数据上训练VinVL-base
  - API-Caption ：使用Microsoft Azure tagging API
  - GT-Caption ：给定COCO原本的Caption
  - Caption+Tags ：链接图片的caption以及tag list标签列表   
4. 选择不同的examples的方式（CLIP model用于相似度计算）
  - Random
  - Question
  - QuestionDissimilar
  - Question+Answer（oracle）
  - Image only
  - Image + Question（文章中使用 PICa-Full）
5. 多查询集成实验，n*k个样例得到k个prompt，每个prompt包含n个例子，得到k个预测的答案，选择log可能值最高的作为最终的答案
6. 以及在VQAv2上的结果，虽然效果不如Oscar好，指明未来方向"期望end-to-end的视觉编码器能帮助更好地回答这些问题"
### 启发
PICa 其实很简单，但是由于“首次使用GPT-3做多模态的任务”，并且效果在OK-VQA数据集上有很大提升，论文中也进行了多个实验来支撑整篇文章，内容丰满，所有有一定的影响力。
### 借鉴价值
文章主要就4个点，1、图片用Caption表示，2、图片通过tagging API获得到的Tags，3、选取不同样例多次查询，选择最好答案，4、example的选择。个人感觉infoseek的数据集关注的是更细粒度的问题，每个问题的关注点不一样，所以选取一些样例做few-shots可能效果不会很好，如户保田GPT-4V上few-shots效果会差一些。尝试使用把问题拆解成两个问题，问题是“这个建筑所在的街道的地址是什么”，将问题拆解为，先问“这个建筑是什么”，再问“这个教堂在哪条街道”。看样例。或者根据这个“教堂”去检索相关的知识，拼接到prompt上。
