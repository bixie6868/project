# 目录：
- [Question Aware Vision Transformer for Multimodal Reasoning(CVPR 2024,Amazon)](#question-aware-vision-transformer-for-multimodal-reasoningcvpr-2024amazon)
- [Small Language Models Fine-tuned to Coordinate Larger Language Models Improve Complex Reasoning（EMNLP，2023）](#small-language-models-fine-tuned-to-coordinate-larger-language-models-improve-complex-reasoningemnlp2023)
- [Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering（CVPR 2024，Northeastern University)](#consistency-and-uncertainty-identifying-unreliable-responses-from-black-box-vision-language-models-for-selective-visual-question-answeringcvpr-2024northeastern-university)
- [How to Configure Good In-Context Sequence for Visual Question Answering（CVPR 2024，东南大学）](#how-to-configure-good-in-context-sequence-for-visual-question-answeringcvpr-2024)
# Question Aware Vision Transformer for Multimodal Reasoning(CVPR 2024,Amazon)
## 动机：
* 大部分Vision-Language模型都是由vision encoder以及LLM和对齐模块组成，但忽视了vision encoding和user queries脱钩
* 贡献： 提出QA-ViT，将question-aware嵌入到视觉编码器，重点关注于问题的相关图像方面。
  ![image](https://github.com/bixie6868/project/assets/78329110/887a5ffd-88c2-43fb-8dff-a81c3a255722)
* 总体框架：
  ![image](https://github.com/bixie6868/project/assets/78329110/3bc759c4-9add-4516-a305-898d4e8549ad)
* ![image](https://github.com/bixie6868/project/assets/78329110/45316dbe-cb9f-4467-8631-8815d5efde43)
# Small Language Models Fine-tuned to Coordinate Larger Language Models Improve Complex Reasoning（EMNLP，2023）
![image](https://github.com/bixie6868/project/assets/78329110/64e17749-d17b-448f-8eed-4012c749dadb)
# Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering（CVPR 2024，Northeastern University）
## 动机：
让模型在无法回答问题时，选择弃权。像GPT-4这种黑盒无法到模型内部去进行更改。
## 贡献：
模型在可视化问题附近的响应的一致性将表明可靠性   
![image](https://github.com/bixie6868/project/assets/78329110/84bcf5b6-a139-4b11-9b01-981891fa633d)
## 实验部分
* 训练模型VQG，可以根据V和A生成问题    
* ![image](https://github.com/bixie6868/project/assets/78329110/dcfd047e-e0f0-41c6-9360-5777cb7dd459)
* **实验结果：**
* ![image](https://github.com/bixie6868/project/assets/78329110/b5052394-e50e-4cb9-88fb-d579cfbc848a)

# How to Configure Good In-Context Sequence for Visual Question Answering（CVPR 2024，东南大学）
## 动机：
* ICL经常通过随机选择来配置上下文序列，因此导致不理想或者次优的结果
## 贡献：
* 探索多样的上下文配置，以找到最有效的配置
* 探究了不同的retrieval策略的效果
* ![image](https://github.com/bixie6868/project/assets/78329110/2c271c4f-e724-4cda-ba92-bfab5dee4e96)

# 想法
* 观察到很多数据集都是基于相同的图片集，如CoCo图片集上就有很多VQA的数据集，这些数据集中的问题有些问题简单，有些问题困难。考虑是否有些问题可以用来辅助回答一些复杂的问题，即作为复杂问题的子问题，帮助获得复杂问题的答案。（这个框架可以作为直接预测或者是利用得到的数据集训练小模型）


