# 目录
- [Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs (CVPR 2024)](#wiki-llava-hierarchical-retrieval-augmented-generation-for-multimodal-llms-cvpr-2024)
- [SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM](#snapntell-enhancing-entity-centric-visual-question-answering-with-retrieval-augmented-multimodal-llm)
- [Fully Authentic Visual Question Answering Dataset from Online Communities](#fully-authentic-visual-question-answering-dataset-from-online-communities)
## Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs (CVPR 2024)
### 动机：
* 没有动机，专注于引入外部知识来回答问题，开发专门用于处理外部知识的架构，而不是提出一个新颖的框架或者vision-and-language adapters
* **关注数据集** ： MLLM处理与外部数据相关的Query能力的评估（需要引入外部知识的评估） Infoseek 和 Encyclopedic-VQA   
* **号称** ： 第一个利用外部资源检索能力的MLLM
### WiKi-LLaVA 框架细节
* ![wiki](https://github.com/bixie6868/project/blob/main/images/QQ%E5%9B%BE%E7%89%8720240429090819.png "wiki")
* ![image](https://github.com/bixie6868/project/assets/78329110/38c740a0-8efd-4892-afa8-f649f7b25af4)
* 从文档中提取（文档、图像、文本标题）
  - 第一阶段 ： 输入图像I，使用文档标题作为可检索键，在外部知识中进行近似k-最近邻搜索。
    ![image](https://github.com/bixie6868/project/assets/78329110/49d11365-8a81-441b-8ce4-d9383e6eb732)   
    知识检索器返回与上述过程检索到的最相关项目相关联的前k个文档
  - 第二阶段 ： 分析每个检索到的文档，识别和用户问题最相关的段落   
    使用Contriever框架嵌入每个选定文档的块，计算嵌入之间的内积作为相似性，每个文档检索n个最适当的段落，得到 K*n个段落
* 最终Prompt设计，丰富了上下文：
   - ![image](https://github.com/bixie6868/project/assets/78329110/31f91bbc-3cbb-4d29-bda6-4c50553b878a)
### 训练：
* 上述方法可以在零样本方式工作，但同时也研究了 微调模型的情况， 增强其利用检索到的段落的能力
* 训练目的： 模型在需要外部知识的问题和真实答案对上进行训练
* 问题： 潜在降低MLLM在不需外部知识的任务上的能力
* 解决 ： 采用数据混合的方法，这种方法中，需要外部知识的事实对与不需要外部知识的事实对在同一小批量中混合。
* **实现细节**：
  - 基于LLaVA微调 ： LoRA微调 使用Encyclopedic-VQA数据及LLaVA-Instruct视觉指令调整数据及nfoSeek训练集。
  - 检索 ：采用了近似的_k_NN搜索而不是精确的_k_NN搜索，使用了Faiss库和一个基于图的HNSW索引，每个顶点32个链接。
### 实验：
  - Encyclopedic-VQA ： 使用该训练集1M来微调LLaVA模型，在5.8k训练集上进行删选掉双跳问题，得到4750个测试数据。知识库：提供2M维基百科知识库
  - Infoseek ： 在val集上进行实验。知识库：提供了6M维基百科实体组成的知识库，随机提取了100k实体子集，确保包含数据集问题相关的11k实体
### 实验结果：
![image](https://github.com/bixie6868/project/assets/78329110/82b4095e-8bbc-4e31-9a1f-b5132684774f)
![image](https://github.com/bixie6868/project/assets/78329110/50b37488-241c-4736-a12e-2c6ca9e9e659)
* 当使用外部知识库时，准确度结果显著提高，特别是在有10万个可检索项的InfoSeek数据集上。然而，CLIP模型在较大知识库中检索正确实体的有限性能导致准确度分数略有下降。这是由于提供给MLLM的附加外部上下文中的噪声文本段落，这些文本段落与不同实体相关，通常不包含信息性内容。
* 结果证实了直接使用检索到的段落来增强预训练MLLM的有效性，并进一步强调了拥有一个好的实体检索模型以限制向MLLM提供不相关内容的重要性。
* ![image](https://github.com/bixie6868/project/assets/78329110/938f9f52-a01c-4715-9fc8-acf74e852b5e)
* ![image](https://github.com/bixie6868/project/assets/78329110/397d55b0-f9e9-48b0-aee1-d868691a0be5)
## SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM
### 动机：
* 目前问题对于处理包含长尾实体的问题，可能产生错误或幻觉反应
* 提出数据集SnapNTell，数据特点：包含很大部分实体，同时包含在图片以及在答案中明确命名。需要额外知识的QA对。22个主类别，7568独一无二实体，每个实体10张图，10个QA对。
### 贡献：
* 提出一个新任务：以实体为中心的VQA，以识别实体并产生对实体的深刻理解。提出模型：一个RAG多模态模型。提出数据集（组合oven和infoseek，识别细粒度实体以及需要推理理解的问题）。
### 数据介绍 : 
* ![image](https://github.com/bixie6868/project/assets/78329110/66d84c09-06ad-43a3-a799-45acda8a8969)
* ![image](https://github.com/bixie6868/project/assets/78329110/4b669ef2-4ef8-4649-8a8a-1b0a33e2e5a1)
### 模型框架 : 
![image](https://github.com/bixie6868/project/assets/78329110/8dc0293d-25a7-44f2-9d48-d3e7ba232097)
### 实验结果：
* 在本文提出的数据集上的效果：
* ![image](https://github.com/bixie6868/project/assets/78329110/dd7d3a08-8602-48f3-9580-115b0fe255cb)
* 解决长尾问题：
* ![image](https://github.com/bixie6868/project/assets/78329110/06bd8b5c-c156-40d4-bcf4-57f582e03520)
* 不同数据集上实验结果：
* ![image](https://github.com/bixie6868/project/assets/78329110/553c68fe-ee1a-4201-a927-a2b77daf0e44)
* 没有放文章提出的模型在这些基准上的准确率？规避问题
* 人工评估1000样例
* ![image](https://github.com/bixie6868/project/assets/78329110/bca7d71f-da0d-44ca-8cd8-3475be266ca4)
### 启发：为了使模型更好的产生子问题，可以构建一个当前问题相似子问题的数据集。实现方式可以使用一个图片查询与该图片相似的图片，并检查该问题是否和当前问题相关度高，信息熵高。(目的：更好的产生子问题）使用构建好的数据集训练一个模型，模型可以更好的产生子问题。   最终使用能产生更好子问题的模型，对原始问题产生一些子问题，并回答该子问题拼接到原问题后面来更好的回答问题。
## Fully Authentic Visual Question Answering Dataset from Online Communities
### 主要内容：
* 介绍一个名为OnlineVQA的数据集，特点：所有内容都来自真实的场景，该数据集的答案较长。
* ![image](https://github.com/bixie6868/project/assets/78329110/ec89227f-f84d-468c-8e9b-e8fd8eca86f5)




