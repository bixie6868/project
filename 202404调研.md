## Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs (CVPR 2024)
### 动机：
* 没有动机，专注于引入外部知识来回答问题，开发专门用于处理外部知识的架构，而不是提出一个新颖的框架或者vision-and-language adapters
* **关注数据集** ： MLLM处理与外部数据相关的Query能力的评估（需要引入外部知识的评估） Infoseek 和 Encyclopedic-VQA   
* **号称** ： 第一个利用外部资源检索能力的MLLM
### WiKi-LLaVA 框架细节
* ![wiki](https://github.com/bixie6868/project/blob/main/images/QQ%E5%9B%BE%E7%89%8720240429090819.png "wiki")
* ![image](https://github.com/bixie6868/project/assets/78329110/38c740a0-8efd-4892-afa8-f649f7b25af4)
* 从文档中提取（文档、图像、文本标题）
  - 第一阶段 ： 输入图像I，使用文档标题作为可检索键，在外部知识中进行近似k-最近邻搜索。
    ![image](https://github.com/bixie6868/project/assets/78329110/49d11365-8a81-441b-8ce4-d9383e6eb732)   
    知识检索器返回与上述过程检索到的最相关项目相关联的前k个文档
  - 第二阶段 ： 分析每个检索到的文档，识别和用户问题最相关的段落   
    使用Contriever框架嵌入每个选定文档的块，计算嵌入之间的内积作为相似性，每个文档检索n个最适当的段落，得到 K*n个段落
* 最终Prompt设计，丰富了上下文：
   - ![image](https://github.com/bixie6868/project/assets/78329110/31f91bbc-3cbb-4d29-bda6-4c50553b878a)
### 训练：
* 上述方法可以在零样本方式工作，但同时也研究了 微调模型的情况， 增强其利用检索到的段落的能力
* 训练目的： 模型在需要外部知识的问题和真实答案对上进行训练
* 问题： 潜在降低MLLM在不需外部知识的任务上的能力
* 解决 ： 采用数据混合的方法，这种方法中，需要外部知识的事实对与不需要外部知识的事实对在同一小批量中混合。
### 实验：
  - Encyclopedic-VQA ： 使用该训练集1M来微调LLaVA模型，在5.8k训练集上进行删选掉双跳问题，得到4750个测试数据。知识库：提供2M维基百科知识库
  - Infoseek ： 在val集上进行实验。知识库：提供了6M维基百科实体组成的知识库，随机提取了100k实体子集，确保包含数据集问题相关的11k实体
  - 
