## Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs (CVPR 2024)
### 动机：
* 没有动机，专注于引入外部知识来回答问题，开发专门用于处理外部知识的架构，而不是提出一个新颖的框架或者vision-and-language adapters
* **关注数据集** ： MLLM处理与外部数据相关的Query能力的评估（需要引入外部知识的评估） Infoseek 和 Encyclopedic-VQA   
* **号称** ： 第一个利用外部资源检索能力的MLLM
### WiKi-LLaVA 框架细节
* ![wiki](https://github.com/bixie6868/project/blob/main/images/QQ%E5%9B%BE%E7%89%8720240429090819.png "wiki")
* ![image](https://github.com/bixie6868/project/assets/78329110/38c740a0-8efd-4892-afa8-f649f7b25af4)
* 从文档中提取（文档、图像、文本标题）
  - 第一阶段 ： 输入图像I，使用文档标题作为可检索键，在外部知识中进行近似k-最近邻搜索。
    ![image](https://github.com/bixie6868/project/assets/78329110/49d11365-8a81-441b-8ce4-d9383e6eb732)
    识检索器返回与上述过程检索到的最相关项目相关联的前 k 个文档
