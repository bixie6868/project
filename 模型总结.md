## BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
### 介绍：
* 两个阶段：
  - 1、从冻结图像编码器学习视觉语言表示
  - 2、基于冻结语言模型，进行视觉到语言生成学习
  - 通过轻量级两阶段预训练模型Querying Transformer缩小模态之间gap
    ![blip](https://github.com/bixie6868/project/blob/main/images/Snipaste_2024-01-17_16-08-06.png "blip")
## InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning
* InstructBLIP 的架构和 BLIP-2 相似，从预训练好的 BLIP-2 模型初始化，由图像编码器、LLM 和 Q-Former 组成。在指令微调期间只训练 Q-Former，冻结图像编码器和 LLM 的参数。作者将26个数据集转化成指令微调的格式，把它们分成13个 held-in 数据集用于指令微调，和13个 held-out 数据集用于 Zero-Shot 能力的评估。
* InstructBLIP 生成的回复更加准确到位，可以通过自适应调整响应长度来直接解决用户意图
### prompt-tuning、Instruct-tuning以及 fine-tunning的区别？
* Prompt-tuning: **调整目标**：主要是修改用于引导模型推理的提示 (prompt) 内容，使之更好地引导模型输出符合预期。**实现方式**： 不改变预训练模型的参数，只调整提示的内容和格式。例如，可以添加额外的信息、修改措辞或者使用不同的模板来引导模型更好地完成特定任务。适用于零样本 (zero-shot) 和少样本 (few-shot) 学习场景。 **缺点：**
对提示设计依赖性高，需要精心设计有效的提示。
模型的泛化能力可能受到限制，难以处理与提示风格差异较大的输入。
* Instruct-tuning:**调整目标**： 同时调整提示和预训练模型参数，使模型能够更好地理解和执行指令 (instruction) 所表达的任务意图。**实现方式**： 将指令和任务相关文本作为额外的训练数据，与预训练数据一起微调模型参数。**缺点：**
需要更多的训练数据，尤其是与指令相关的文本。
调整过程更复杂，计算成本更高。
* Fine-tuning:**调整目标**： 直接修改预训练模型的参数，使其适应特定的下游任务。**实现方式**： 使用下游任务的训练数据微调模型参数，调整幅度较大，可以改变模型内部的表达和推理机制。**缺点：**
需要大量下游任务的训练数据，对数据需求更大。
可能破坏预训练模型的泛化能力，难以应用于其他任务。
调整过程更复杂，计算成本更高。
